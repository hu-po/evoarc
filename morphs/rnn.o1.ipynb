{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "#<config>\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.environ.get(\"SEED\", 0))\n",
    "    # --- data\n",
    "    data_seed: int = 42\n",
    "    train_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "    train_solutions: str = '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "    valid_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n",
    "    valid_solutions: str = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'\n",
    "    submission_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "    num_order_augs: int = 1 # number of task train grid order augmentations\n",
    "    num_color_augs: int = 1 # number of task grid color augmentations (maintains matching train and test pair)\n",
    "    max_grid_size: int = 30 # maximum grid size\n",
    "    max_train_pairs: int = 10 # maximum number of train pairs\n",
    "    max_test_pairs: int = 3 # maximum number of test pairs\n",
    "    num_colors: int = 10 # number of colors in the grid\n",
    "    pad_value: int = 0 # padding value for grids\n",
    "    # --- logging \n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = \"hug\"\n",
    "    wandb_project: str = \"evoarc\"\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # --- model\n",
    "    # TODO: add model hyperparameters here\n",
    "    # --- training\n",
    "    num_epochs: int = 8 # number of epochs to train\n",
    "    batch_size: int = 2 # number of tasks per batch\n",
    "    print_every: int = 1e4 # print training loss every this many steps\n",
    "    early_stopping_patience: int = 10 # stop training if no improvement for this many epochs\n",
    "    learning_rate: float = 1e-3 # initial learning rate\n",
    "#</config>\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if cfg.compute_backend == \"kaggle\":\n",
    "    # when submitting to kaggle, save the output to the current directory\n",
    "    output_dir = os.getcwd()\n",
    "else:\n",
    "    output_dir = f\"/evoarc/output/{cfg.morph}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"output_dir: {output_dir}\")\n",
    "print(f\"config:{json.dumps(cfg.__dict__, indent=4)}\")\n",
    "config_filepath = os.path.join(output_dir, \"config.json\")\n",
    "with open(config_filepath, 'w') as f:\n",
    "    json.dump(cfg.__dict__, f, indent=4)\n",
    "\n",
    "if not cfg.compute_backend == \"kaggle\":\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    wandb.init(entity=cfg.wandb_entity, project=cfg.wandb_project, name=f\"{cfg.compute_backend}.{cfg.morph}.{str(uuid.uuid4())[:6]}\", config=cfg.__dict__)\n",
    "    wandb.save(config_filepath)\n",
    "\n",
    "def save_checkpoint(params, filename):\n",
    "    with open(os.path.join(output_dir, filename), 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    with open(os.path.join(output_dir, filename), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "#<data>\n",
    "def load_tasks(challenges_path: str, solutions_path: str, cfg: Config):\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    if solutions_path is not None:\n",
    "        with open(solutions_path, 'r') as f:\n",
    "            solutions_dict = json.load(f)\n",
    "        print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    \"\"\"\n",
    "    tasks are stored in JSON format. Each JSON file consists of two key-value pairs.\n",
    "    train: a list of two to ten input/output pairs (typically three.) These are used for your algorithm to infer a rule.\n",
    "    test: a list of one to three input/output pairs (typically one.) Your model should apply the inferred rule from the train set and construct an output solution.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for task_id in challenges_dict.keys():\n",
    "        train_in = []\n",
    "        train_out = []\n",
    "        test_in = []\n",
    "        test_out = []\n",
    "        \"\"\"\n",
    "        a \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive).\n",
    "        the smallest possible grid size is 1x1 and the largest is 30x30.\n",
    "        0 represents the background color, 1-9 represent the pattern colors.\n",
    "        \"\"\"\n",
    "        for pair in challenges_dict[task_id]['train']:\n",
    "            train_in.append(np.array(pair['input'], dtype=np.uint8))\n",
    "            train_out.append(np.array(pair['output'], dtype=np.uint8))\n",
    "        for grid in challenges_dict[task_id]['test']:\n",
    "            test_in.append(np.array(grid['input'], dtype=np.uint8))\n",
    "        if solutions_path is not None:\n",
    "            for grid in solutions_dict[task_id]:\n",
    "                test_out.append(np.array(grid, dtype=np.uint8))\n",
    "        tasks.append((task_id, train_in, train_out, test_in, test_out))\n",
    "    return tasks\n",
    "\n",
    "def augmentation(tasks, cfg: Config):\n",
    "    augmented_tasks = []\n",
    "    # grid structure means we can use spatial symmetry to augment the tasks\n",
    "    for task in tasks:\n",
    "        task_id, train_in, train_out, eval_in, eval_out = task\n",
    "        for aug in [\n",
    "            np.fliplr,\n",
    "            np.flipud,\n",
    "            lambda x: np.rot90(x, k=1),\n",
    "            lambda x: np.rot90(x, k=3)\n",
    "        ]:\n",
    "            augmented_tasks.append((\n",
    "                f\"{task_id}.s{str(uuid.uuid4())[:6]}\",\n",
    "                [aug(grid) for grid in train_in],\n",
    "                [aug(grid) for grid in train_out],\n",
    "                [aug(grid) for grid in eval_in],\n",
    "                [aug(grid) for grid in eval_out]\n",
    "            ))\n",
    "    print(f\"after spatial augmentation, tasks count: {len(tasks)}\")\n",
    "    # assume order of train grids is also a valid augmentation\n",
    "    for _ in range(cfg.num_order_augs):\n",
    "        _augmented_tasks = []\n",
    "        for task in augmented_tasks:\n",
    "            task_id, train_in, train_out, eval_in, eval_out = task\n",
    "            train_order = np.random.permutation(len(train_in))\n",
    "            _augmented_tasks.append((\n",
    "                f\"{task_id}.o{str(uuid.uuid4())[:6]}\",\n",
    "                [train_in[i] for i in train_order],\n",
    "                [train_out[i] for i in train_order],\n",
    "                eval_in,\n",
    "                eval_out\n",
    "            ))\n",
    "        augmented_tasks.extend(_augmented_tasks)\n",
    "    print(f\"after order augmentation x{cfg.num_order_augs}, tasks count: {len(tasks)}\")\n",
    "    # all colors (except for background) are interchangeable (but must match entire set)\n",
    "    for _ in range(cfg.num_color_augs):\n",
    "        _augmented_tasks = []\n",
    "        for task in augmented_tasks:\n",
    "            task_id, train_in, train_out, eval_in, eval_out = task\n",
    "            color_map = np.arange(10)\n",
    "            color_map[1:] = np.random.permutation(color_map[1:])\n",
    "            _augmented_tasks.append((\n",
    "                f\"{task_id}.c{str(uuid.uuid4())[:6]}\",\n",
    "                [np.take(color_map, grid) for grid in train_in],\n",
    "                [np.take(color_map, grid) for grid in train_out],\n",
    "                [np.take(color_map, grid) for grid in eval_in],\n",
    "                [np.take(color_map, grid) for grid in eval_out]\n",
    "            ))\n",
    "        augmented_tasks.extend(_augmented_tasks)\n",
    "    print(f\"after color augmentation x{cfg.num_color_augs}, tasks count: {len(tasks)}\")\n",
    "    return augmented_tasks\n",
    "\n",
    "def pad_grids(grids, cfg: Config):\n",
    "    padded_grids = []\n",
    "    for grid in grids:\n",
    "        h, w = grid.shape\n",
    "        padded_grid = np.full((cfg.max_grid_size, cfg.max_grid_size), cfg.pad_value, dtype=np.uint8)\n",
    "        padded_grid[:h, :w] = grid\n",
    "        padded_grids.append(padded_grid)\n",
    "    return np.stack(padded_grids)\n",
    "\n",
    "def pad_pairs(pairs, pad_len: int, cfg: Config):\n",
    "    while len(pairs) < pad_len:\n",
    "        pairs.append(np.full((cfg.max_grid_size, cfg.max_grid_size), cfg.pad_value, dtype=np.uint8))\n",
    "    return np.stack(pairs)\n",
    "\n",
    "def datagen(tasks, cfg: Config, mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        tasks = augmentation(tasks, cfg)\n",
    "        np.random.shuffle(tasks)\n",
    "    num_tasks = len(tasks)\n",
    "    if mode == \"train\":\n",
    "        batch_size = cfg.batch_size\n",
    "        num_batches = num_tasks // batch_size\n",
    "    elif mode == \"valid\":\n",
    "        batch_size = cfg.batch_size\n",
    "        num_batches = (num_tasks + batch_size - 1) // batch_size\n",
    "    elif mode == \"submission\":\n",
    "        batch_size = 1\n",
    "        num_batches = num_tasks\n",
    "    else:\n",
    "        raise ValueError(f\"invalid mode: {mode}\")\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_tasks)\n",
    "        batch_tasks = tasks[start_idx:end_idx]\n",
    "        # skip last incomplete batch during training\n",
    "        if mode == \"train\" and len(batch_tasks) < batch_size:\n",
    "            continue\n",
    "        batch_task_id, batch_train_in, batch_train_out, batch_test_in, batch_test_out = [], [], [], [], []\n",
    "        for task in batch_tasks:\n",
    "            task_id, train_in, train_out, test_in, test_out = task\n",
    "            batch_task_id.append(task_id)\n",
    "            # Pad the grids and pairs as before\n",
    "            train_in_padded = pad_grids(train_in, cfg)\n",
    "            train_out_padded = pad_grids(train_out, cfg)\n",
    "            test_in_padded = pad_grids(test_in, cfg)\n",
    "            train_in_padded = pad_pairs(list(train_in_padded), cfg.max_train_pairs, cfg)\n",
    "            train_out_padded = pad_pairs(list(train_out_padded), cfg.max_train_pairs, cfg)\n",
    "            test_in_padded = pad_pairs(list(test_in_padded), cfg.max_test_pairs, cfg)\n",
    "            batch_train_in.append(train_in_padded)\n",
    "            batch_train_out.append(train_out_padded)\n",
    "            batch_test_in.append(test_in_padded)\n",
    "            if test_out:\n",
    "                test_out_padded = pad_grids(test_out, cfg)\n",
    "                test_out_padded = pad_pairs(list(test_out_padded), cfg.max_test_pairs, cfg)\n",
    "                batch_test_out.append(test_out_padded)\n",
    "            else:\n",
    "                batch_test_out.append(np.zeros_like(test_in_padded))\n",
    "        yield (\n",
    "            batch_task_id,\n",
    "            jnp.array(batch_train_in, dtype=jnp.float32),\n",
    "            jnp.array(batch_train_out, dtype=jnp.float32),\n",
    "            jnp.array(batch_test_in, dtype=jnp.float32),\n",
    "            jnp.array(batch_test_out, dtype=jnp.float32),\n",
    "        )\n",
    "#</data>\n",
    "\n",
    "#<model>\n",
    "def init_params(key, cfg: Config):\n",
    "    \"\"\"\n",
    "    Initialize model parameters, putting them on the GPU.\n",
    "    \"\"\"\n",
    "    key, subkey = jax.random.split(key)\n",
    "    embedding_dim = 32  # Embedding dimension\n",
    "    num_colors = cfg.num_colors  # Number of colors (10)\n",
    "\n",
    "    # Embedding matrix\n",
    "    embed_params = jax.random.normal(subkey, (num_colors, embedding_dim))\n",
    "\n",
    "    # RNN parameters\n",
    "    rnn_hidden_size = 64  # Hidden size for RNN\n",
    "    hidden_dim = rnn_hidden_size\n",
    "\n",
    "    def init_gru_params(key, input_dim, hidden_dim):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        Wz = jax.random.normal(subkey, (input_dim + hidden_dim, hidden_dim))\n",
    "        bz = jnp.zeros((hidden_dim,))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        Wr = jax.random.normal(subkey, (input_dim + hidden_dim, hidden_dim))\n",
    "        br = jnp.zeros((hidden_dim,))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        Wh = jax.random.normal(subkey, (input_dim + hidden_dim, hidden_dim))\n",
    "        bh = jnp.zeros((hidden_dim,))\n",
    "        return {'Wz': Wz, 'bz': bz, 'Wr': Wr, 'br': br, 'Wh': Wh, 'bh': bh}\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    encoder_params = init_gru_params(subkey, embedding_dim, hidden_dim)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    decoder_params = init_gru_params(subkey, embedding_dim, hidden_dim)\n",
    "\n",
    "    # Output layer parameters\n",
    "    key, subkey = jax.random.split(key)\n",
    "    W_out = jax.random.normal(subkey, (hidden_dim, num_colors))\n",
    "    b_out = jnp.zeros((num_colors,))\n",
    "\n",
    "    # Task embedding projection\n",
    "    key, subkey = jax.random.split(key)\n",
    "    W_task = jax.random.normal(subkey, (hidden_dim * 2, hidden_dim))\n",
    "    b_task = jnp.zeros((hidden_dim,))\n",
    "\n",
    "    params = {\n",
    "        'embed': embed_params,\n",
    "        'encoder': encoder_params,\n",
    "        'decoder': decoder_params,\n",
    "        'W_out': W_out,\n",
    "        'b_out': b_out,\n",
    "        'W_task': W_task,\n",
    "        'b_task': b_task\n",
    "    }\n",
    "\n",
    "    return params\n",
    "\n",
    "def gru_cell(params, h_prev, x):\n",
    "    \"\"\"\n",
    "    GRU cell implementation.\n",
    "    \"\"\"\n",
    "    Wz, bz = params['Wz'], params['bz']\n",
    "    Wr, br = params['Wr'], params['br']\n",
    "    Wh, bh = params['Wh'], params['bh']\n",
    "    concat = jnp.concatenate([x, h_prev], axis=-1)\n",
    "    z = jax.nn.sigmoid(jnp.dot(concat, Wz) + bz)\n",
    "    r = jax.nn.sigmoid(jnp.dot(concat, Wr) + br)\n",
    "    concat_r = jnp.concatenate([x, r * h_prev], axis=-1)\n",
    "    h_tilde = jnp.tanh(jnp.dot(concat_r, Wh) + bh)\n",
    "    h = (1 - z) * h_prev + z * h_tilde\n",
    "    return h\n",
    "\n",
    "def model(params, train_in, train_out, test_in):\n",
    "    \"\"\"\n",
    "    Forward pass of the model.\n",
    "    \"\"\"\n",
    "    batch_size = train_in.shape[0]\n",
    "    max_train_pairs = train_in.shape[1]\n",
    "    max_test_pairs = test_in.shape[1]\n",
    "    max_grid_size = train_in.shape[2]\n",
    "    seq_len = max_grid_size * max_grid_size\n",
    "\n",
    "    num_colors = cfg.num_colors\n",
    "\n",
    "    embed_params = params['embed']\n",
    "    encoder_params = params['encoder']\n",
    "    decoder_params = params['decoder']\n",
    "    W_out = params['W_out']\n",
    "    b_out = params['b_out']\n",
    "    W_task = params['W_task']\n",
    "    b_task = params['b_task']\n",
    "\n",
    "    embedding_dim = embed_params.shape[1]\n",
    "    hidden_dim = W_out.shape[0]\n",
    "\n",
    "    # Process grids\n",
    "    def process_grid(grid):\n",
    "        grid_flat = grid.reshape(-1, seq_len)\n",
    "        grid_embed = embed_params[grid_flat.astype(jnp.int32)]\n",
    "        return grid_embed\n",
    "\n",
    "    # Encode train_in and train_out grids\n",
    "    train_in_embed = process_grid(train_in)\n",
    "    train_out_embed = process_grid(train_out)\n",
    "\n",
    "    def encode_sequence(embed_seq, rnn_params):\n",
    "        h = jnp.zeros((embed_seq.shape[0], hidden_dim))\n",
    "        for t in range(embed_seq.shape[1]):\n",
    "            x_t = embed_seq[:, t, :]\n",
    "            h = gru_cell(rnn_params, h, x_t)\n",
    "        return h\n",
    "\n",
    "    train_in_enc = encode_sequence(train_in_embed, encoder_params)\n",
    "    train_out_enc = encode_sequence(train_out_embed, encoder_params)\n",
    "\n",
    "    # Concatenate train_in and train_out embeddings\n",
    "    train_pairs_enc = jnp.concatenate([train_in_enc, train_out_enc], axis=-1)\n",
    "    train_pairs_enc = train_pairs_enc.reshape(batch_size, max_train_pairs, -1)\n",
    "\n",
    "    # Compute task embedding\n",
    "    task_embedding = jnp.mean(train_pairs_enc, axis=1)\n",
    "\n",
    "    # Process test_in grids\n",
    "    test_in_embed = process_grid(test_in)\n",
    "\n",
    "    # Initial hidden state for decoder\n",
    "    task_embedding_expanded = jnp.repeat(task_embedding[:, None, :], max_test_pairs, axis=1)\n",
    "    task_embedding_flat = task_embedding_expanded.reshape(-1, hidden_dim * 2)\n",
    "    h0 = jnp.tanh(jnp.dot(task_embedding_flat, W_task) + b_task)\n",
    "\n",
    "    # Decode sequence\n",
    "    def decode_sequence(h, embed_seq, rnn_params):\n",
    "        outputs = []\n",
    "        for t in range(embed_seq.shape[1]):\n",
    "            x_t = embed_seq[:, t, :]\n",
    "            h = gru_cell(rnn_params, h, x_t)\n",
    "            y_t = jnp.dot(h, W_out) + b_out\n",
    "            outputs.append(y_t)\n",
    "        outputs = jnp.stack(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    outputs = decode_sequence(h0, test_in_embed, decoder_params)\n",
    "    outputs = outputs.reshape(batch_size, max_test_pairs, seq_len, num_colors)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def loss_fn(model_output, test_out):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss between model outputs and ground truth outputs.\n",
    "    \"\"\"\n",
    "    batch_size = test_out.shape[0]\n",
    "    max_test_pairs = test_out.shape[1]\n",
    "    seq_len = test_out.shape[2] * test_out.shape[3]\n",
    "\n",
    "    # Flatten test_out\n",
    "    test_out_flat = test_out.reshape(batch_size, max_test_pairs, -1).astype(jnp.int32)\n",
    "\n",
    "    # Compute cross-entropy loss\n",
    "    logits = model_output\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, test_out_flat)\n",
    "    loss = jnp.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(model_output, test_out):\n",
    "    \"\"\"\n",
    "    Compute exact match accuracy.\n",
    "    \"\"\"\n",
    "    batch_size = test_out.shape[0]\n",
    "    max_test_pairs = test_out.shape[1]\n",
    "    seq_len = test_out.shape[2] * test_out.shape[3]\n",
    "\n",
    "    # Flatten test_out\n",
    "    test_out_flat = test_out.reshape(batch_size, max_test_pairs, -1).astype(jnp.int32)\n",
    "\n",
    "    # Predicted labels\n",
    "    preds = jnp.argmax(model_output, axis=-1)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    correct = jnp.all(preds == test_out_flat, axis=-1)\n",
    "    accuracy = jnp.mean(correct.astype(jnp.float32))\n",
    "    return accuracy\n",
    "\n",
    "def predict_fn(model_output):\n",
    "    \"\"\"\n",
    "    Convert model output to test output for submission.\n",
    "    \"\"\"\n",
    "    preds = jnp.argmax(model_output, axis=-1)\n",
    "    batch_size = preds.shape[0]\n",
    "    max_test_pairs = preds.shape[1]\n",
    "    seq_len = preds.shape[2]\n",
    "    max_grid_size = int(jnp.sqrt(seq_len))\n",
    "\n",
    "    # Reshape predictions to grids\n",
    "    preds_grids = preds.reshape(batch_size, max_test_pairs, max_grid_size, max_grid_size)\n",
    "    preds_grids = np.array(preds_grids)\n",
    "    return preds_grids\n",
    "#</model>\n",
    "\n",
    "\n",
    "#<training>\n",
    "import optax\n",
    "\n",
    "np.random.seed(cfg.data_seed)\n",
    "key = jax.random.PRNGKey(cfg.seed)\n",
    "params = init_params(key, cfg)\n",
    "opt = optax.adam(cfg.learning_rate)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, train_in, train_out, test_in, test_out):\n",
    "    def loss_and_grad(params):\n",
    "        model_output = model(params, train_in, train_out, test_in)\n",
    "        loss = loss_fn(model_output, test_out)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_and_grad)(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "@jax.jit\n",
    "def validation_step(params, train_in, train_out, test_in, test_out):\n",
    "    model_output = model(params, train_in, train_out, test_in)\n",
    "    loss = loss_fn(model_output, test_out)\n",
    "    accuracy = accuracy_fn(model_output, test_out)\n",
    "    return loss, accuracy\n",
    "\n",
    "train_tasks = load_tasks(cfg.train_challenges, cfg.train_solutions, cfg)\n",
    "valid_tasks = load_tasks(cfg.valid_challenges, cfg.valid_solutions, cfg)\n",
    "global_step = 0\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{cfg.num_epochs}\")\n",
    "    for _, train_in, train_out, test_in, test_out in datagen(train_tasks, cfg, mode=\"train\"):\n",
    "        global_step += cfg.batch_size\n",
    "        params, opt_state, train_loss = train_step(params, opt_state, train_in, train_out, test_in, test_out)\n",
    "        if global_step % cfg.print_every == 0:\n",
    "            print(f\"global step {global_step} loss = {train_loss.item():.4f}\")\n",
    "            if cfg.compute_backend != \"kaggle\":\n",
    "                wandb.log({\"train_loss\": train_loss.item()}, step=global_step)\n",
    "    # validation\n",
    "    total_valid_loss = 0.0\n",
    "    total_valid_acc = 0.0\n",
    "    num_batches = 0\n",
    "    for _, train_in, train_out, test_in, test_out in datagen(valid_tasks, cfg, mode=\"valid\"):\n",
    "        batch_valid_loss, batch_valid_acc = validation_step(params, train_in, train_out, test_in, test_out)\n",
    "        total_valid_loss += batch_valid_loss.item()\n",
    "        total_valid_acc += batch_valid_acc.item()\n",
    "        num_batches += 1\n",
    "    valid_loss = total_valid_loss / num_batches\n",
    "    valid_acc = total_valid_acc / num_batches\n",
    "    print(f'valid_loss: {valid_loss:.4f}, valid_acc: {valid_acc:.4f}')\n",
    "    if not cfg.compute_backend == \"kaggle\":\n",
    "        wandb.log({\"valid_loss\": valid_loss, \"valid_acc\": valid_acc}, step=global_step)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_valid_acc = valid_acc\n",
    "        epochs_without_improvement = 0\n",
    "        save_checkpoint(params, \"best.pkl\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= cfg.early_stopping_patience:\n",
    "            print(f\"early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "save_checkpoint(params, \"final.pkl\")\n",
    "# submission will be made with two model checkpoints\n",
    "attempt1_ckpt = \"best.pkl\"\n",
    "attempt2_ckpt = \"final.pkl\"\n",
    "#</training>\n",
    "\n",
    "\"\"\"\n",
    "for each task output in the evaluation set, you should make exactly 2 predictions (attempt_1, attempt_2).\n",
    "most tasks only have a single output (a single dictionary enclosed in a list), although some tasks have multiple outputs that must be predicted.\n",
    "when a task has multiple test outputs that need to be predicted, they must be in the same order as the corresponding test inputs.\n",
    "\"\"\"\n",
    "submission_tasks = load_tasks(cfg.submission_challenges, None, cfg)\n",
    "predictions = {}\n",
    "for i, ckpt in enumerate([attempt1_ckpt, attempt2_ckpt]):\n",
    "    params = load_checkpoint(ckpt)\n",
    "    for task_id, train_in, train_out, test_in, _ in datagen(submission_tasks, cfg, mode=\"submission\"):\n",
    "        model_output = model(params, train_in, train_out, test_in)\n",
    "        test_out = predict_fn(model_output)\n",
    "        for b, id in enumerate(task_id):\n",
    "            if id not in predictions:\n",
    "                predictions[id] = []\n",
    "            predictions[id].append({f\"attempt_{i+1}\" : test_out[b].tolist()})\n",
    "submission_filepath = os.path.join(output_dir, \"submission.json\")\n",
    "\n",
    "with open(submission_filepath, 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "results = {\"accuracy\": best_valid_acc, \"loss\": best_valid_loss}\n",
    "results_filepath = os.path.join(output_dir, \"results.json\")\n",
    "\n",
    "with open(results_filepath, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "if not cfg.compute_backend == \"kaggle\":\n",
    "    wandb.save(submission_filepath)\n",
    "    wandb.save(results_filepath)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
